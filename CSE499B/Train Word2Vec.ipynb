{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Word2Vec.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2qNoh05Avplx"},"outputs":[],"source":["import re  # For preprocessing\n","import pandas as pd  # For data handling\n","from time import time  # To time our operations\n","from collections import defaultdict  # For word frequency\n","\n","import spacy  # For preprocessing\n","\n","import logging  # Setting up the loggings to monitor gensim\n","logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"vLuhVI3xmMlP","executionInfo":{"status":"ok","timestamp":1650396828817,"user_tz":-360,"elapsed":25796,"user":{"displayName":"Jubayer Hossain Arnob 1813124","userId":"00675389204231916688"}},"outputId":"f38f9275-ed7e-4f30-e6aa-fdb5835139a6","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from csv import reader\n","# read csv file as a list of lists\n","with open('/content/coco.csv', 'r') as read_obj:\n","    # pass the file object to reader() to get the reader object\n","    csv_reader = reader(read_obj)\n","    # Pass reader object to list() to get a list of lists\n","    list_of_rows = list(csv_reader)\n","    print(list_of_rows)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JdMM65ISsR9h","outputId":"f7d9f78f-9ac3-4e64-b002-e7e953676cef","executionInfo":{"status":"ok","timestamp":1650396834212,"user_tz":-360,"elapsed":850,"user":{"displayName":"Jubayer Hossain Arnob 1813124","userId":"00675389204231916688"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["IOPub data rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_data_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]}]},{"cell_type":"code","source":["from gensim.models import Word2Vec"],"metadata":{"id":"13CyIoy5nzal"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"z38UyuZ9t12-"}},{"cell_type":"code","source":["model = Word2Vec(list_of_rows, min_count=1)\n","print(model)"],"metadata":{"id":"6a1nKFRn5n3T","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a0ef3b47-fc03-47a1-be8b-7a90bfd80956","executionInfo":{"status":"ok","timestamp":1650396857323,"user_tz":-360,"elapsed":4981,"user":{"displayName":"Jubayer Hossain Arnob 1813124","userId":"00675389204231916688"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO - 19:34:11: collecting all words and their counts\n","INFO - 19:34:11: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","INFO - 19:34:11: PROGRESS: at sentence #10000, processed 210000 words, keeping 135 word types\n","INFO - 19:34:11: PROGRESS: at sentence #20000, processed 420000 words, keeping 135 word types\n","INFO - 19:34:12: PROGRESS: at sentence #30000, processed 630000 words, keeping 135 word types\n","INFO - 19:34:12: PROGRESS: at sentence #40000, processed 840000 words, keeping 135 word types\n","INFO - 19:34:12: PROGRESS: at sentence #50000, processed 1050000 words, keeping 135 word types\n","INFO - 19:34:12: PROGRESS: at sentence #60000, processed 1260000 words, keeping 135 word types\n","INFO - 19:34:12: PROGRESS: at sentence #70000, processed 1470000 words, keeping 135 word types\n","INFO - 19:34:12: PROGRESS: at sentence #80000, processed 1680000 words, keeping 135 word types\n","INFO - 19:34:12: PROGRESS: at sentence #90000, processed 1890000 words, keeping 135 word types\n","INFO - 19:34:12: PROGRESS: at sentence #100000, processed 2100000 words, keeping 135 word types\n","INFO - 19:34:12: collected 135 word types from a corpus of 2150967 raw words and 102427 sentences\n","INFO - 19:34:12: Loading a fresh vocabulary\n","INFO - 19:34:12: effective_min_count=1 retains 135 unique words (100% of original 135, drops 0)\n","INFO - 19:34:12: effective_min_count=1 leaves 2150967 word corpus (100% of original 2150967, drops 0)\n","INFO - 19:34:12: deleting the raw counts dictionary of 135 items\n","INFO - 19:34:12: sample=0.001 downsamples 22 most-common words\n","INFO - 19:34:12: downsampling leaves estimated 482346 word corpus (22.4% of prior 2150967)\n","INFO - 19:34:12: estimated required memory for 135 words and 100 dimensions: 175500 bytes\n","INFO - 19:34:12: resetting layer weights\n","INFO - 19:34:12: training model with 3 workers on 135 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n","INFO - 19:34:13: worker thread finished; awaiting finish of 2 more threads\n","INFO - 19:34:13: worker thread finished; awaiting finish of 1 more threads\n","INFO - 19:34:13: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:34:13: EPOCH - 1 : training on 2150967 raw words (483275 effective words) took 0.9s, 556911 effective words/s\n","INFO - 19:34:14: worker thread finished; awaiting finish of 2 more threads\n","INFO - 19:34:14: worker thread finished; awaiting finish of 1 more threads\n","INFO - 19:34:14: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:34:14: EPOCH - 2 : training on 2150967 raw words (482653 effective words) took 0.9s, 558250 effective words/s\n","INFO - 19:34:14: worker thread finished; awaiting finish of 2 more threads\n","INFO - 19:34:14: worker thread finished; awaiting finish of 1 more threads\n","INFO - 19:34:14: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:34:14: EPOCH - 3 : training on 2150967 raw words (482925 effective words) took 0.9s, 554361 effective words/s\n","INFO - 19:34:15: worker thread finished; awaiting finish of 2 more threads\n","INFO - 19:34:15: worker thread finished; awaiting finish of 1 more threads\n","INFO - 19:34:15: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:34:15: EPOCH - 4 : training on 2150967 raw words (482356 effective words) took 0.9s, 548878 effective words/s\n","INFO - 19:34:16: worker thread finished; awaiting finish of 2 more threads\n","INFO - 19:34:16: worker thread finished; awaiting finish of 1 more threads\n","INFO - 19:34:16: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:34:16: EPOCH - 5 : training on 2150967 raw words (482488 effective words) took 0.9s, 564887 effective words/s\n","INFO - 19:34:16: training on a 10754835 raw words (2413697 effective words) took 4.4s, 546903 effective words/s\n"]},{"output_type":"stream","name":"stdout","text":["Word2Vec(vocab=135, size=100, alpha=0.025)\n"]}]},{"cell_type":"code","source":["words = list(model.wv.vocab)\n","print(words)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J8mkYKWVzu6G","outputId":"a96ee204-7db5-40f4-c76a-40ae3c2afd57","executionInfo":{"status":"ok","timestamp":1650396986514,"user_tz":-360,"elapsed":331,"user":{"displayName":"Jubayer Hossain Arnob 1813124","userId":"00675389204231916688"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['person', 'umbrella', 'boat', 'traffic light', 'river', 'water', 'tree', 'pavement', 'building', '', 'sky', 'street', 'zebra', 'mountain', 'grass', 'banana', 'handbag', 'dirt', 'food', 'motorcycle', 'truck', 'suitcase', 'bus', 'parking meter', 'road', 'wallbrick', 'fire hydrant', 'car', 'toothbrush', 'book', 'cardboard', 'shelf', 'stairs', 'wallwood', 'floor', 'wall', 'giraffe', 'stop sign', 'railroad', 'train', 'gravel', 'cow', 'bird', 'elephant', 'bench', 'fence', 'surfboard', 'sea', 'potted plant', 'cat', 'curtain', 'window', 'table', 'paper', 'skis', 'snow', 'cup', 'apple', 'dining table', 'sports ball', 'baseball glove', 'baseball bat', 'playingfield', 'house', 'dog', 'bowl', 'couch', 'floorwood', 'ceiling', 'cabinet', 'rug', 'chair', 'tv', 'keyboard', 'mouse', 'cell phone', 'kite', 'sand', 'bed', 'remote', 'toilet', 'walltile', 'orange', 'fruit', 'horse', 'roof', 'refrigerator', 'microwave', 'pizza', 'skateboard', 'bottle', 'sheep', 'fork', 'wine glass', 'sandwich', 'vase', 'clock', 'frisbee', 'platform', 'broccoli', 'carrot', 'laptop', 'sink', 'knife', 'counter', 'blanket', 'bicycle', 'banner', 'tent', 'backpack', 'tennis racket', 'flower', 'rock', 'doorstuff', 'mirrorstuff', 'cake', 'teddy bear', 'airplane', 'oven', 'donut', 'bear', 'bridge', 'light', 'hot dog', 'net', 'towel', 'spoon', 'snowboard', 'windowblind', 'toaster', 'wallstone', 'scissors', 'tie', 'hair drier', 'pillow']\n"]}]},{"cell_type":"code","source":["word_freq = defaultdict(int)\n","for sent in list_of_rows:\n","    for i in sent:\n","        word_freq[i] += 1\n","len(word_freq)\n","sorted(word_freq, key=word_freq.get, reverse=True)[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-bKe0YcT69aR","outputId":"a6cbfb14-8ab7-4cda-8f1b-a6196aa3feb5","executionInfo":{"status":"ok","timestamp":1650396990776,"user_tz":-360,"elapsed":514,"user":{"displayName":"Jubayer Hossain Arnob 1813124","userId":"00675389204231916688"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['',\n"," 'person',\n"," 'wall',\n"," 'tree',\n"," 'sky',\n"," 'building',\n"," 'grass',\n"," 'pavement',\n"," 'table',\n"," 'road']"]},"metadata":{},"execution_count":113}]},{"cell_type":"code","source":["import multiprocessing\n","from gensim.models import Word2Vec\n","\n","cores = multiprocessing.cpu_count()"],"metadata":{"id":"5TZPAABK1YPI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2v_model = Word2Vec(min_count=1,\n","                     window=3,\n","                     size=600,\n","                     sample=6e-05, \n","                     alpha=0.03, \n","                     min_alpha=0.0007, \n","                     negative=20,\n","                     workers=cores-1)"],"metadata":{"id":"w7J8s5sG3PGA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["t = time()\n","\n","w2v_model.build_vocab(list_of_rows, progress_per=10000)\n","\n","print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-4lrM_cW3Z73","outputId":"89e3b20c-46dd-4527-de04-b7381e28ef6b","executionInfo":{"status":"ok","timestamp":1650397031375,"user_tz":-360,"elapsed":517,"user":{"displayName":"Jubayer Hossain Arnob 1813124","userId":"00675389204231916688"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO - 19:37:10: collecting all words and their counts\n","INFO - 19:37:10: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","INFO - 19:37:10: PROGRESS: at sentence #10000, processed 210000 words, keeping 135 word types\n","INFO - 19:37:10: PROGRESS: at sentence #20000, processed 420000 words, keeping 135 word types\n","INFO - 19:37:10: PROGRESS: at sentence #30000, processed 630000 words, keeping 135 word types\n","INFO - 19:37:10: PROGRESS: at sentence #40000, processed 840000 words, keeping 135 word types\n","INFO - 19:37:10: PROGRESS: at sentence #50000, processed 1050000 words, keeping 135 word types\n","INFO - 19:37:10: PROGRESS: at sentence #60000, processed 1260000 words, keeping 135 word types\n","INFO - 19:37:10: PROGRESS: at sentence #70000, processed 1470000 words, keeping 135 word types\n","INFO - 19:37:10: PROGRESS: at sentence #80000, processed 1680000 words, keeping 135 word types\n","INFO - 19:37:10: PROGRESS: at sentence #90000, processed 1890000 words, keeping 135 word types\n","INFO - 19:37:10: PROGRESS: at sentence #100000, processed 2100000 words, keeping 135 word types\n","INFO - 19:37:10: collected 135 word types from a corpus of 2150967 raw words and 102427 sentences\n","INFO - 19:37:10: Loading a fresh vocabulary\n","INFO - 19:37:10: effective_min_count=1 retains 135 unique words (100% of original 135, drops 0)\n","INFO - 19:37:10: effective_min_count=1 leaves 2150967 word corpus (100% of original 2150967, drops 0)\n","INFO - 19:37:10: deleting the raw counts dictionary of 135 items\n","INFO - 19:37:10: sample=6e-05 downsamples 131 most-common words\n","INFO - 19:37:10: downsampling leaves estimated 118282 word corpus (5.5% of prior 2150967)\n","INFO - 19:37:10: estimated required memory for 135 words and 600 dimensions: 715500 bytes\n","INFO - 19:37:10: resetting layer weights\n"]},{"output_type":"stream","name":"stdout","text":["Time to build vocab: 0.01 mins\n"]}]},{"cell_type":"code","source":["t = time()\n","\n","w2v_model.train(list_of_rows, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)\n","\n","print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n","\n","w2v_model.init_sims(replace=True)\n","#w2v_model.save(\"word2vec.model\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LeochXES358H","outputId":"37f9461f-3189-4450-a12e-02deb159072e","executionInfo":{"status":"ok","timestamp":1650397156023,"user_tz":-360,"elapsed":119868,"user":{"displayName":"Jubayer Hossain Arnob 1813124","userId":"00675389204231916688"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO - 19:37:15: training model with 1 workers on 135 vocabulary and 600 features, using sg=0 hs=0 sample=6e-05 negative=20 window=3\n","INFO - 19:37:16: EPOCH 1 - PROGRESS: at 83.65% examples, 98035 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:37:16: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:16: EPOCH - 1 : training on 2150967 raw words (118193 effective words) took 1.2s, 97245 effective words/s\n","INFO - 19:37:17: EPOCH 2 - PROGRESS: at 83.19% examples, 97828 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:18: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:18: EPOCH - 2 : training on 2150967 raw words (118160 effective words) took 1.2s, 98139 effective words/s\n","INFO - 19:37:19: EPOCH 3 - PROGRESS: at 84.11% examples, 99257 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:37:19: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:19: EPOCH - 3 : training on 2150967 raw words (118464 effective words) took 1.2s, 99210 effective words/s\n","INFO - 19:37:20: EPOCH 4 - PROGRESS: at 81.33% examples, 95936 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:20: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:20: EPOCH - 4 : training on 2150967 raw words (118731 effective words) took 1.2s, 96323 effective words/s\n","INFO - 19:37:21: EPOCH 5 - PROGRESS: at 85.04% examples, 100729 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:21: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:21: EPOCH - 5 : training on 2150967 raw words (118317 effective words) took 1.2s, 100436 effective words/s\n","INFO - 19:37:22: EPOCH 6 - PROGRESS: at 84.58% examples, 99683 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:22: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:22: EPOCH - 6 : training on 2150967 raw words (118443 effective words) took 1.2s, 99467 effective words/s\n","INFO - 19:37:23: EPOCH 7 - PROGRESS: at 85.04% examples, 100438 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:37:24: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:24: EPOCH - 7 : training on 2150967 raw words (118607 effective words) took 1.2s, 100167 effective words/s\n","INFO - 19:37:25: EPOCH 8 - PROGRESS: at 85.04% examples, 100624 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:25: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:25: EPOCH - 8 : training on 2150967 raw words (118380 effective words) took 1.2s, 100836 effective words/s\n","INFO - 19:37:26: EPOCH 9 - PROGRESS: at 85.51% examples, 100809 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:26: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:26: EPOCH - 9 : training on 2150967 raw words (118418 effective words) took 1.2s, 100733 effective words/s\n","INFO - 19:37:27: EPOCH 10 - PROGRESS: at 85.04% examples, 100734 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:27: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:27: EPOCH - 10 : training on 2150967 raw words (118484 effective words) took 1.2s, 99893 effective words/s\n","INFO - 19:37:28: EPOCH 11 - PROGRESS: at 85.04% examples, 100212 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:37:28: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:28: EPOCH - 11 : training on 2150967 raw words (118515 effective words) took 1.2s, 100471 effective words/s\n","INFO - 19:37:29: EPOCH 12 - PROGRESS: at 85.04% examples, 99987 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:30: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:30: EPOCH - 12 : training on 2150967 raw words (118202 effective words) took 1.2s, 100199 effective words/s\n","INFO - 19:37:31: EPOCH 13 - PROGRESS: at 84.58% examples, 99579 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:37:31: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:31: EPOCH - 13 : training on 2150967 raw words (118006 effective words) took 1.2s, 99660 effective words/s\n","INFO - 19:37:32: EPOCH 14 - PROGRESS: at 85.97% examples, 101461 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:32: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:32: EPOCH - 14 : training on 2150967 raw words (118024 effective words) took 1.2s, 101391 effective words/s\n","INFO - 19:37:33: EPOCH 15 - PROGRESS: at 85.04% examples, 100191 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:33: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:33: EPOCH - 15 : training on 2150967 raw words (118399 effective words) took 1.2s, 100343 effective words/s\n","INFO - 19:37:34: EPOCH 16 - PROGRESS: at 84.58% examples, 99473 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:34: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:34: EPOCH - 16 : training on 2150967 raw words (118104 effective words) took 1.2s, 99302 effective words/s\n","INFO - 19:37:35: EPOCH 17 - PROGRESS: at 84.58% examples, 99592 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:36: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:36: EPOCH - 17 : training on 2150967 raw words (118095 effective words) took 1.2s, 98876 effective words/s\n","INFO - 19:37:37: EPOCH 18 - PROGRESS: at 84.58% examples, 100151 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:37:37: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:37: EPOCH - 18 : training on 2150967 raw words (118645 effective words) took 1.2s, 100030 effective words/s\n","INFO - 19:37:38: EPOCH 19 - PROGRESS: at 85.04% examples, 100329 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:38: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:38: EPOCH - 19 : training on 2150967 raw words (118526 effective words) took 1.2s, 100462 effective words/s\n","INFO - 19:37:39: EPOCH 20 - PROGRESS: at 85.51% examples, 100671 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:39: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:39: EPOCH - 20 : training on 2150967 raw words (118174 effective words) took 1.2s, 100575 effective words/s\n","INFO - 19:37:40: EPOCH 21 - PROGRESS: at 84.11% examples, 98978 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:37:40: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:40: EPOCH - 21 : training on 2150967 raw words (118655 effective words) took 1.2s, 98656 effective words/s\n","INFO - 19:37:41: EPOCH 22 - PROGRESS: at 85.04% examples, 100375 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:42: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:42: EPOCH - 22 : training on 2150967 raw words (118395 effective words) took 1.2s, 100229 effective words/s\n","INFO - 19:37:43: EPOCH 23 - PROGRESS: at 84.58% examples, 98820 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:43: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:43: EPOCH - 23 : training on 2150967 raw words (117662 effective words) took 1.2s, 98299 effective words/s\n","INFO - 19:37:44: EPOCH 24 - PROGRESS: at 84.11% examples, 99108 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:44: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:44: EPOCH - 24 : training on 2150967 raw words (118334 effective words) took 1.2s, 99289 effective words/s\n","INFO - 19:37:45: EPOCH 25 - PROGRESS: at 83.65% examples, 98677 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:37:45: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:45: EPOCH - 25 : training on 2150967 raw words (118440 effective words) took 1.2s, 98413 effective words/s\n","INFO - 19:37:46: EPOCH 26 - PROGRESS: at 84.58% examples, 99573 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:46: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:46: EPOCH - 26 : training on 2150967 raw words (118127 effective words) took 1.2s, 99590 effective words/s\n","INFO - 19:37:47: EPOCH 27 - PROGRESS: at 83.65% examples, 98359 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:48: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:48: EPOCH - 27 : training on 2150967 raw words (118369 effective words) took 1.2s, 98684 effective words/s\n","INFO - 19:37:49: EPOCH 28 - PROGRESS: at 85.04% examples, 100227 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:49: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:49: EPOCH - 28 : training on 2150967 raw words (118335 effective words) took 1.2s, 99627 effective words/s\n","INFO - 19:37:50: EPOCH 29 - PROGRESS: at 84.11% examples, 99057 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:37:50: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:50: EPOCH - 29 : training on 2150967 raw words (118714 effective words) took 1.2s, 99411 effective words/s\n","INFO - 19:37:51: EPOCH 30 - PROGRESS: at 85.04% examples, 100162 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:37:51: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:51: EPOCH - 30 : training on 2150967 raw words (118196 effective words) took 1.2s, 99757 effective words/s\n","INFO - 19:37:52: EPOCH 31 - PROGRESS: at 85.51% examples, 100559 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:37:52: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:52: EPOCH - 31 : training on 2150967 raw words (117960 effective words) took 1.2s, 100299 effective words/s\n","INFO - 19:37:53: EPOCH 32 - PROGRESS: at 85.51% examples, 100479 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:37:53: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:53: EPOCH - 32 : training on 2150967 raw words (117900 effective words) took 1.2s, 100165 effective words/s\n","INFO - 19:37:55: EPOCH 33 - PROGRESS: at 84.58% examples, 99918 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:55: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:55: EPOCH - 33 : training on 2150967 raw words (118905 effective words) took 1.2s, 100371 effective words/s\n","INFO - 19:37:56: EPOCH 34 - PROGRESS: at 84.11% examples, 99266 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:37:56: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:56: EPOCH - 34 : training on 2150967 raw words (118793 effective words) took 1.2s, 98850 effective words/s\n","INFO - 19:37:57: EPOCH 35 - PROGRESS: at 85.04% examples, 99340 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:37:57: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:57: EPOCH - 35 : training on 2150967 raw words (117844 effective words) took 1.2s, 99287 effective words/s\n","INFO - 19:37:58: EPOCH 36 - PROGRESS: at 85.51% examples, 100745 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:37:58: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:58: EPOCH - 36 : training on 2150967 raw words (118084 effective words) took 1.2s, 100355 effective words/s\n","INFO - 19:37:59: EPOCH 37 - PROGRESS: at 85.04% examples, 100120 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:37:59: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:37:59: EPOCH - 37 : training on 2150967 raw words (118179 effective words) took 1.2s, 99689 effective words/s\n","INFO - 19:38:00: EPOCH 38 - PROGRESS: at 84.11% examples, 99422 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:01: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:01: EPOCH - 38 : training on 2150967 raw words (118734 effective words) took 1.2s, 99282 effective words/s\n","INFO - 19:38:02: EPOCH 39 - PROGRESS: at 84.11% examples, 99887 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:02: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:02: EPOCH - 39 : training on 2150967 raw words (119063 effective words) took 1.2s, 99212 effective words/s\n","INFO - 19:38:03: EPOCH 40 - PROGRESS: at 84.58% examples, 99812 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:03: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:03: EPOCH - 40 : training on 2150967 raw words (118558 effective words) took 1.2s, 99083 effective words/s\n","INFO - 19:38:04: EPOCH 41 - PROGRESS: at 84.11% examples, 98802 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:04: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:04: EPOCH - 41 : training on 2150967 raw words (118222 effective words) took 1.2s, 98620 effective words/s\n","INFO - 19:38:05: EPOCH 42 - PROGRESS: at 82.72% examples, 97088 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:06: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:06: EPOCH - 42 : training on 2150967 raw words (118198 effective words) took 1.2s, 97189 effective words/s\n","INFO - 19:38:07: EPOCH 43 - PROGRESS: at 84.11% examples, 99514 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:07: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:07: EPOCH - 43 : training on 2150967 raw words (118541 effective words) took 1.2s, 99551 effective words/s\n","INFO - 19:38:08: EPOCH 44 - PROGRESS: at 83.65% examples, 98974 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:08: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:08: EPOCH - 44 : training on 2150967 raw words (118875 effective words) took 1.2s, 99004 effective words/s\n","INFO - 19:38:09: EPOCH 45 - PROGRESS: at 83.65% examples, 98398 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:09: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:09: EPOCH - 45 : training on 2150967 raw words (117931 effective words) took 1.2s, 97815 effective words/s\n","INFO - 19:38:10: EPOCH 46 - PROGRESS: at 85.04% examples, 100773 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:10: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:10: EPOCH - 46 : training on 2150967 raw words (118412 effective words) took 1.2s, 99975 effective words/s\n","INFO - 19:38:11: EPOCH 47 - PROGRESS: at 84.58% examples, 99876 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:12: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:12: EPOCH - 47 : training on 2150967 raw words (118350 effective words) took 1.2s, 99265 effective words/s\n","INFO - 19:38:13: EPOCH 48 - PROGRESS: at 85.04% examples, 100738 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:13: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:13: EPOCH - 48 : training on 2150967 raw words (118505 effective words) took 1.2s, 100818 effective words/s\n","INFO - 19:38:14: EPOCH 49 - PROGRESS: at 84.58% examples, 99362 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:14: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:14: EPOCH - 49 : training on 2150967 raw words (118330 effective words) took 1.2s, 99835 effective words/s\n","INFO - 19:38:15: EPOCH 50 - PROGRESS: at 85.04% examples, 99977 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:15: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:15: EPOCH - 50 : training on 2150967 raw words (117942 effective words) took 1.2s, 99427 effective words/s\n","INFO - 19:38:16: EPOCH 51 - PROGRESS: at 85.04% examples, 100579 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:16: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:16: EPOCH - 51 : training on 2150967 raw words (118281 effective words) took 1.2s, 100832 effective words/s\n","INFO - 19:38:17: EPOCH 52 - PROGRESS: at 85.04% examples, 99910 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:17: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:17: EPOCH - 52 : training on 2150967 raw words (118213 effective words) took 1.2s, 99087 effective words/s\n","INFO - 19:38:18: EPOCH 53 - PROGRESS: at 85.04% examples, 100311 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:19: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:19: EPOCH - 53 : training on 2150967 raw words (118029 effective words) took 1.2s, 100081 effective words/s\n","INFO - 19:38:20: EPOCH 54 - PROGRESS: at 83.19% examples, 97678 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:20: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:20: EPOCH - 54 : training on 2150967 raw words (118398 effective words) took 1.2s, 98559 effective words/s\n","INFO - 19:38:21: EPOCH 55 - PROGRESS: at 86.44% examples, 101795 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:21: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:21: EPOCH - 55 : training on 2150967 raw words (118163 effective words) took 1.2s, 101601 effective words/s\n","INFO - 19:38:22: EPOCH 56 - PROGRESS: at 84.58% examples, 99182 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:22: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:22: EPOCH - 56 : training on 2150967 raw words (117815 effective words) took 1.2s, 99227 effective words/s\n","INFO - 19:38:23: EPOCH 57 - PROGRESS: at 85.04% examples, 100697 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:23: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:23: EPOCH - 57 : training on 2150967 raw words (118730 effective words) took 1.2s, 100782 effective words/s\n","INFO - 19:38:24: EPOCH 58 - PROGRESS: at 84.58% examples, 99444 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:25: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:25: EPOCH - 58 : training on 2150967 raw words (118075 effective words) took 1.2s, 98768 effective words/s\n","INFO - 19:38:26: EPOCH 59 - PROGRESS: at 83.19% examples, 98150 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:26: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:26: EPOCH - 59 : training on 2150967 raw words (118340 effective words) took 1.2s, 98335 effective words/s\n","INFO - 19:38:27: EPOCH 60 - PROGRESS: at 84.58% examples, 99150 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:27: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:27: EPOCH - 60 : training on 2150967 raw words (117860 effective words) took 1.2s, 99741 effective words/s\n","INFO - 19:38:28: EPOCH 61 - PROGRESS: at 86.44% examples, 101515 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:28: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:28: EPOCH - 61 : training on 2150967 raw words (118121 effective words) took 1.2s, 101352 effective words/s\n","INFO - 19:38:29: EPOCH 62 - PROGRESS: at 84.11% examples, 98902 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:29: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:29: EPOCH - 62 : training on 2150967 raw words (117831 effective words) took 1.2s, 99289 effective words/s\n","INFO - 19:38:30: EPOCH 63 - PROGRESS: at 85.04% examples, 99844 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:31: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:31: EPOCH - 63 : training on 2150967 raw words (118059 effective words) took 1.2s, 99806 effective words/s\n","INFO - 19:38:32: EPOCH 64 - PROGRESS: at 84.58% examples, 99659 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:32: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:32: EPOCH - 64 : training on 2150967 raw words (118273 effective words) took 1.2s, 98536 effective words/s\n","INFO - 19:38:33: EPOCH 65 - PROGRESS: at 84.11% examples, 99165 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:33: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:33: EPOCH - 65 : training on 2150967 raw words (118189 effective words) took 1.2s, 98935 effective words/s\n","INFO - 19:38:34: EPOCH 66 - PROGRESS: at 84.58% examples, 98948 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:34: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:34: EPOCH - 66 : training on 2150967 raw words (117687 effective words) took 1.2s, 98856 effective words/s\n","INFO - 19:38:35: EPOCH 67 - PROGRESS: at 84.11% examples, 99025 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:35: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:35: EPOCH - 67 : training on 2150967 raw words (118301 effective words) took 1.2s, 98934 effective words/s\n","INFO - 19:38:36: EPOCH 68 - PROGRESS: at 84.58% examples, 100205 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:37: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:37: EPOCH - 68 : training on 2150967 raw words (118537 effective words) took 1.2s, 99958 effective words/s\n","INFO - 19:38:38: EPOCH 69 - PROGRESS: at 85.04% examples, 100405 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:38: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:38: EPOCH - 69 : training on 2150967 raw words (118353 effective words) took 1.2s, 100624 effective words/s\n","INFO - 19:38:39: EPOCH 70 - PROGRESS: at 84.58% examples, 99539 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:39: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:39: EPOCH - 70 : training on 2150967 raw words (118086 effective words) took 1.2s, 98968 effective words/s\n","INFO - 19:38:40: EPOCH 71 - PROGRESS: at 84.58% examples, 99523 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:40: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:40: EPOCH - 71 : training on 2150967 raw words (118272 effective words) took 1.2s, 98716 effective words/s\n","INFO - 19:38:41: EPOCH 72 - PROGRESS: at 83.65% examples, 98205 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:41: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:41: EPOCH - 72 : training on 2150967 raw words (118027 effective words) took 1.2s, 98424 effective words/s\n","INFO - 19:38:42: EPOCH 73 - PROGRESS: at 84.11% examples, 99805 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:43: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:43: EPOCH - 73 : training on 2150967 raw words (119086 effective words) took 1.2s, 99801 effective words/s\n","INFO - 19:38:44: EPOCH 74 - PROGRESS: at 85.97% examples, 101496 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:44: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:44: EPOCH - 74 : training on 2150967 raw words (118372 effective words) took 1.2s, 101259 effective words/s\n","INFO - 19:38:45: EPOCH 75 - PROGRESS: at 85.51% examples, 101091 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:45: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:45: EPOCH - 75 : training on 2150967 raw words (118211 effective words) took 1.2s, 101248 effective words/s\n","INFO - 19:38:46: EPOCH 76 - PROGRESS: at 83.65% examples, 99075 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:46: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:46: EPOCH - 76 : training on 2150967 raw words (118711 effective words) took 1.2s, 98375 effective words/s\n","INFO - 19:38:47: EPOCH 77 - PROGRESS: at 85.04% examples, 101019 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:47: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:47: EPOCH - 77 : training on 2150967 raw words (118767 effective words) took 1.2s, 100919 effective words/s\n","INFO - 19:38:48: EPOCH 78 - PROGRESS: at 83.65% examples, 99058 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:49: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:49: EPOCH - 78 : training on 2150967 raw words (118548 effective words) took 1.2s, 98862 effective words/s\n","INFO - 19:38:50: EPOCH 79 - PROGRESS: at 84.58% examples, 99370 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:50: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:50: EPOCH - 79 : training on 2150967 raw words (118204 effective words) took 1.2s, 99710 effective words/s\n","INFO - 19:38:51: EPOCH 80 - PROGRESS: at 85.51% examples, 100856 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:51: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:51: EPOCH - 80 : training on 2150967 raw words (118475 effective words) took 1.2s, 101013 effective words/s\n","INFO - 19:38:52: EPOCH 81 - PROGRESS: at 85.97% examples, 101026 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:52: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:52: EPOCH - 81 : training on 2150967 raw words (118082 effective words) took 1.2s, 100988 effective words/s\n","INFO - 19:38:53: EPOCH 82 - PROGRESS: at 84.11% examples, 99009 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:53: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:53: EPOCH - 82 : training on 2150967 raw words (118162 effective words) took 1.2s, 98535 effective words/s\n","INFO - 19:38:54: EPOCH 83 - PROGRESS: at 85.51% examples, 100737 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:54: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:54: EPOCH - 83 : training on 2150967 raw words (118256 effective words) took 1.2s, 100580 effective words/s\n","INFO - 19:38:55: EPOCH 84 - PROGRESS: at 85.04% examples, 100567 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:56: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:56: EPOCH - 84 : training on 2150967 raw words (118566 effective words) took 1.2s, 100563 effective words/s\n","INFO - 19:38:57: EPOCH 85 - PROGRESS: at 84.58% examples, 99826 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:57: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:57: EPOCH - 85 : training on 2150967 raw words (118598 effective words) took 1.2s, 99789 effective words/s\n","INFO - 19:38:58: EPOCH 86 - PROGRESS: at 85.04% examples, 100401 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:38:58: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:58: EPOCH - 86 : training on 2150967 raw words (118077 effective words) took 1.2s, 100223 effective words/s\n","INFO - 19:38:59: EPOCH 87 - PROGRESS: at 85.51% examples, 100534 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:38:59: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:38:59: EPOCH - 87 : training on 2150967 raw words (117981 effective words) took 1.2s, 100271 effective words/s\n","INFO - 19:39:00: EPOCH 88 - PROGRESS: at 84.58% examples, 99698 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:39:00: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:39:00: EPOCH - 88 : training on 2150967 raw words (118575 effective words) took 1.2s, 99089 effective words/s\n","INFO - 19:39:01: EPOCH 89 - PROGRESS: at 84.58% examples, 99948 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:39:02: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:39:02: EPOCH - 89 : training on 2150967 raw words (118771 effective words) took 1.2s, 100269 effective words/s\n","INFO - 19:39:03: EPOCH 90 - PROGRESS: at 85.04% examples, 100613 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:39:03: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:39:03: EPOCH - 90 : training on 2150967 raw words (118648 effective words) took 1.2s, 100660 effective words/s\n","INFO - 19:39:04: EPOCH 91 - PROGRESS: at 85.04% examples, 100048 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:39:04: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:39:04: EPOCH - 91 : training on 2150967 raw words (118103 effective words) took 1.2s, 100218 effective words/s\n","INFO - 19:39:05: EPOCH 92 - PROGRESS: at 84.58% examples, 99635 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:39:05: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:39:05: EPOCH - 92 : training on 2150967 raw words (118652 effective words) took 1.2s, 99546 effective words/s\n","INFO - 19:39:06: EPOCH 93 - PROGRESS: at 85.04% examples, 100524 words/s, in_qsize 1, out_qsize 0\n","INFO - 19:39:06: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:39:06: EPOCH - 93 : training on 2150967 raw words (118225 effective words) took 1.2s, 100351 effective words/s\n","INFO - 19:39:07: EPOCH 94 - PROGRESS: at 85.04% examples, 100138 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:39:08: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:39:08: EPOCH - 94 : training on 2150967 raw words (118259 effective words) took 1.2s, 99281 effective words/s\n","INFO - 19:39:09: EPOCH 95 - PROGRESS: at 85.51% examples, 101003 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:39:09: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:39:09: EPOCH - 95 : training on 2150967 raw words (118094 effective words) took 1.2s, 100726 effective words/s\n","INFO - 19:39:10: EPOCH 96 - PROGRESS: at 85.04% examples, 99560 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:39:10: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:39:10: EPOCH - 96 : training on 2150967 raw words (117758 effective words) took 1.2s, 99630 effective words/s\n","INFO - 19:39:11: EPOCH 97 - PROGRESS: at 85.04% examples, 99989 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:39:11: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:39:11: EPOCH - 97 : training on 2150967 raw words (118141 effective words) took 1.2s, 100160 effective words/s\n","INFO - 19:39:12: EPOCH 98 - PROGRESS: at 84.58% examples, 99966 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:39:12: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:39:12: EPOCH - 98 : training on 2150967 raw words (118350 effective words) took 1.2s, 100153 effective words/s\n","INFO - 19:39:13: EPOCH 99 - PROGRESS: at 85.04% examples, 100230 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:39:14: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:39:14: EPOCH - 99 : training on 2150967 raw words (118111 effective words) took 1.2s, 100125 effective words/s\n","INFO - 19:39:15: EPOCH 100 - PROGRESS: at 84.11% examples, 99184 words/s, in_qsize 2, out_qsize 0\n","INFO - 19:39:15: worker thread finished; awaiting finish of 0 more threads\n","INFO - 19:39:15: EPOCH - 100 : training on 2150967 raw words (118294 effective words) took 1.2s, 98719 effective words/s\n","INFO - 19:39:15: training on a 215096700 raw words (11831157 effective words) took 119.5s, 98984 effective words/s\n","INFO - 19:39:15: precomputing L2-norms of word weight vectors\n"]},{"output_type":"stream","name":"stdout","text":["Time to train the model: 1.99 mins\n"]}]},{"cell_type":"code","source":["w2v_model.save(\"word2vec.model\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vm8POwgzZEyR","executionInfo":{"status":"ok","timestamp":1650398096884,"user_tz":-360,"elapsed":331,"user":{"displayName":"Jubayer Hossain Arnob 1813124","userId":"00675389204231916688"}},"outputId":"94e3f153-4dbe-4900-ed0f-732346af7647"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO - 19:54:56: saving Word2Vec object under word2vec.model, separately None\n","INFO - 19:54:56: not storing attribute vectors_norm\n","INFO - 19:54:56: not storing attribute cum_table\n","INFO - 19:54:56: saved word2vec.model\n"]}]},{"cell_type":"code","source":["w2v_model = Word2Vec.load(\"/content/word2vec.model\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_eKismTIBvwm","executionInfo":{"status":"ok","timestamp":1650398121317,"user_tz":-360,"elapsed":321,"user":{"displayName":"Jubayer Hossain Arnob 1813124","userId":"00675389204231916688"}},"outputId":"22a920e4-42b8-4f8f-b13c-ff05e7c498fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO - 19:55:20: loading Word2Vec object from /content/word2vec.model\n","INFO - 19:55:20: loading wv recursively from /content/word2vec.model.wv.* with mmap=None\n","INFO - 19:55:20: setting ignored attribute vectors_norm to None\n","INFO - 19:55:20: loading vocabulary recursively from /content/word2vec.model.vocabulary.* with mmap=None\n","INFO - 19:55:20: loading trainables recursively from /content/word2vec.model.trainables.* with mmap=None\n","INFO - 19:55:20: setting ignored attribute cum_table to None\n","INFO - 19:55:20: loaded /content/word2vec.model\n"]}]},{"cell_type":"code","source":["w2v_model.wv.doesnt_match(['traffic light', 'sand', 'sky', 'grass', 'dirt'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"id":"fcZk-O2e6yqe","outputId":"165e5ebd-a9e1-43ba-b5f2-1fdc34be5a40","executionInfo":{"status":"ok","timestamp":1650398316243,"user_tz":-360,"elapsed":332,"user":{"displayName":"Jubayer Hossain Arnob 1813124","userId":"00675389204231916688"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO - 19:58:35: precomputing L2-norms of word weight vectors\n","/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n","  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"]},{"output_type":"execute_result","data":{"text/plain":["'sand'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":129}]},{"cell_type":"code","source":["import numpy as np\n","import gensim\n","from gensim.models import word2vec,KeyedVectors\n","from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"xRIv4jGtEGAR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim.test.utils import common_texts\n","from gensim.models import Word2Vec\n","model = Word2Vec.load('/content/word2vec.model')\n"],"metadata":{"id":"_t_j1Qm8HxdZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.train([[\"sea\", \"boat\", \"ship\", \"water\"]], total_examples=1, epochs=5)\n","(0, 2)\n"],"metadata":{"id":"45_ar1cEIem4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vector = model.wv['dirt']\n","model.wv.most_similar('dirt', topn=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xtSnAN-5IszN","executionInfo":{"status":"ok","timestamp":1650398334300,"user_tz":-360,"elapsed":527,"user":{"displayName":"Jubayer Hossain Arnob 1813124","userId":"00675389204231916688"}},"outputId":"2df6fa70-356c-4ba5-d8c4-106a0a1a7eb4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('giraffe', 0.5471502542495728),\n"," ('zebra', 0.5439437627792358),\n"," ('sheep', 0.5411251783370972),\n"," ('bear', 0.4755692780017853),\n"," ('cow', 0.46511414647102356),\n"," ('frisbee', 0.4624115824699402),\n"," ('net', 0.41908639669418335),\n"," ('elephant', 0.41688260436058044),\n"," ('bird', 0.39757847785949707),\n"," ('kite', 0.36848539113998413)]"]},"metadata":{},"execution_count":130}]},{"cell_type":"code","source":["from gensim.models import KeyedVectors\n","\n","# Store just the words + their trained embeddings.\n","word_vectors = model.wv\n","word_vectors.save(\"word2vec.wordvectors\")\n","\n","# Load back with memory-mapping = read-only, shared across processes.\n","wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n","\n","vector = wv['sky']  # Get numpy vector of a word\n","wv['sky']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cF_i5TFSJfga","executionInfo":{"status":"ok","timestamp":1650398227142,"user_tz":-360,"elapsed":539,"user":{"displayName":"Jubayer Hossain Arnob 1813124","userId":"00675389204231916688"}},"outputId":"b36d1650-5e0d-4c26-d70f-1103db301375"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO - 19:57:06: saving Word2VecKeyedVectors object under word2vec.wordvectors, separately None\n","INFO - 19:57:06: not storing attribute vectors_norm\n","INFO - 19:57:06: saved word2vec.wordvectors\n","INFO - 19:57:06: loading Word2VecKeyedVectors object from word2vec.wordvectors\n","INFO - 19:57:06: setting ignored attribute vectors_norm to None\n","INFO - 19:57:06: loaded word2vec.wordvectors\n"]},{"output_type":"execute_result","data":{"text/plain":["array([ 1.6411704 , -0.7739449 ,  2.1949825 , -0.37812376, -0.24030088,\n","       -2.517047  , -0.32561746,  0.09075115,  0.0700877 , -0.85378087,\n","        0.05490034, -1.0194312 , -1.7656786 , -0.66530085, -0.5557473 ,\n","        0.51925826,  0.38448623,  0.23255853, -1.1548628 ,  0.5650632 ,\n","       -0.8199324 ,  0.77606714,  1.590109  , -1.571565  ,  0.11829256,\n","        0.4551691 ,  0.31297264,  2.4237947 ,  1.2931975 , -0.10863712,\n","       -0.59380716, -1.411561  , -1.8020092 ,  0.93891853, -0.03314521,\n","       -0.3316696 ,  0.55254465,  0.00993329,  1.5621841 , -1.8103073 ,\n","        1.2104567 ,  0.6463522 ,  0.17044015, -1.4449722 ,  1.592907  ,\n","        1.097872  , -0.5049965 , -0.38363272,  0.97954184,  1.4732882 ,\n","        0.5640724 , -1.0357399 , -0.52070004,  1.107016  , -0.42360872,\n","        0.11155544,  0.89180404, -0.41272315,  0.16712077, -2.0958915 ,\n","        0.40467834,  0.56129944,  0.2073999 , -0.7970026 ,  1.0144069 ,\n","       -0.9962594 ,  0.6876469 ,  1.1119717 ,  0.6177037 ,  1.8736169 ,\n","        1.5460601 ,  0.71290547,  0.08706325,  0.41926426,  1.6942638 ,\n","        0.4463397 , -0.9781088 , -0.3359598 ,  1.3846555 ,  1.8863764 ,\n","       -1.2691009 ,  0.05028863,  1.9734694 ,  0.02055076, -0.8653027 ,\n","        0.9715617 , -0.21595822, -1.0235494 , -1.2203236 ,  2.3884807 ,\n","       -0.7138027 ,  1.2034063 ,  0.07329987, -2.078275  ,  0.58461595,\n","        1.51059   , -0.6440787 ,  0.77245754,  0.8054208 ,  0.04335244],\n","      dtype=float32)"]},"metadata":{},"execution_count":123}]},{"cell_type":"code","source":["from gensim.models import Word2Vec, KeyedVectors   \n","model.wv.save_word2vec_format('model.bin', binary=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_YBWGYHNb7Q","executionInfo":{"status":"ok","timestamp":1650398230364,"user_tz":-360,"elapsed":330,"user":{"displayName":"Jubayer Hossain Arnob 1813124","userId":"00675389204231916688"}},"outputId":"0c558d9c-3447-46f3-9513-45a7089c28c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO - 19:57:09: storing 135x100 projection weights into model.bin\n"]}]},{"cell_type":"code","source":["# Load a word2vec model stored in the C *binary* format.\n","vector_word_notations = KeyedVectors.load_word2vec_format('model.bin',binary=True, encoding= 'unicode_escape')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xGYwaJuyJs49","executionInfo":{"status":"ok","timestamp":1650398232573,"user_tz":-360,"elapsed":538,"user":{"displayName":"Jubayer Hossain Arnob 1813124","userId":"00675389204231916688"}},"outputId":"09bb0546-95f8-4faf-cccd-1d522d2cabc5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO - 19:57:11: loading projection weights from model.bin\n","WARNING - 19:57:11: duplicate word 'baseball' in model.bin, ignoring all but first\n","INFO - 19:57:11: duplicate words detected, shrinking matrix size from 135 to 134\n","INFO - 19:57:11: loaded (134, 100) matrix from model.bin\n"]}]},{"cell_type":"markdown","source":[""],"metadata":{"id":"jEVB5NHNHwzc"}},{"cell_type":"code","source":["def odd_word_out(input_words):\n","    '''The function accepts a list of word and returns the odd word.'''\n","     \n","    # Generate all word embeddings for the given list of words\n","     \n","    whole_word_vectors = [vector_word_notations[i] for i in input_words]\n","     \n","    # average vector for all word vectors\n","    mean_vector = np.mean(whole_word_vectors,axis=0)\n","     \n","    # Iterate over every word and find similarity\n","    odd_word = None\n","    odd=[]\n","    for i in input_words:\n","        similarity = cosine_similarity([vector_word_notations[i]],[mean_vector])\n","        if similarity < 0.6:\n","          odd.append(i)\n","        print(\"cosine similarity score between %s and mean_vector is %.3f\"%(i,similarity))\n","\n","    print(odd)"],"metadata":{"id":"xj1CpLmEEcUC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_1 = ['sky','dirt', 'toilet', 'playingfield', 'tree', 'fence','grass' ]\n"],"metadata":{"id":"i2Wa05IGKy2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["odd_word_out(input_1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":311},"id":"uzxKSmawK_9I","executionInfo":{"status":"error","timestamp":1650398784561,"user_tz":-360,"elapsed":8,"user":{"displayName":"Jubayer Hossain Arnob 1813124","userId":"00675389204231916688"}},"outputId":"ea13a9d2-a610-426f-b7f5-145a7ec48000"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-132-c1ada0a2d11c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0modd_word_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-126-be324c656fb4>\u001b[0m in \u001b[0;36modd_word_out\u001b[0;34m(input_words)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Generate all word embeddings for the given list of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mwhole_word_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvector_word_notations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# average vector for all word vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-126-be324c656fb4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Generate all word embeddings for the given list of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mwhole_word_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvector_word_notations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# average vector for all word vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"word 'dirt' not in vocabulary\""]}]}]}
